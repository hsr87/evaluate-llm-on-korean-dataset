{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa01a76",
   "metadata": {},
   "source": [
    "# Compute statistics for all benchmark results and save them as Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a388f7dc-c6a9-483c-ba10-f61d0af921fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed results/[KMMLU] Phi-3-5-MoE-instruct.csv\n",
      "Processed results/[KMMLU] Phi-3-5-MoE-instruct-5shot.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4o-240513-5shot.csv\n",
      "Processed results/[KMMLU] Phi-3-5-mini-instruct.csv\n",
      "Processed results/[HAERAE] llama-3-1-8b-instruct.csv\n",
      "Processed results/[CLIcK] gpt-5.1-chat-2025-11-13.csv\n",
      "Processed results/[KMMLU] gpt-4.1-mini-2025-04-14.csv\n",
      "Processed results/[KMMLU] Phi-4-0shot.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-5-MoE-instruct.csv\n",
      "Processed results/[KMMLU] gpt-5-chat-2025-08-08-0shot.csv\n",
      "Processed results/[KMMLU] gpt-4.1-2025-04-14.csv\n",
      "Processed results/[KMMLU] gpt-5.1-chat-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU] gpt-5-mini-2025-08-08-0shot.csv\n",
      "Processed results/[CLIcK] gpt-4.1-2025-04-14.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-mini-128k-June.csv\n",
      "Processed results/[KMMLU-HARD] Phi-4-0shot.csv\n",
      "Processed results/[CLIcK] Phi-3-mini-128k-June.csv\n",
      "Processed results/[HAERAE] Phi-3-5-mini-instruct.csv\n",
      "Processed results/[KMMLU-HARD] gpt-5.1-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4-turbo-240409-5shot.csv\n",
      "Processed results/[HAERAE] gpt-35-turbo-230613.csv\n",
      "Processed results/[KMMLU] Phi-3-mini-128k-June-5shot.csv\n",
      "Processed results/[HAERAE] gpt-4.1-2025-04-14.csv\n",
      "Processed results/[HAERAE] gpt-oss-120b-2025-08-05.csv\n",
      "Processed results/[CLIcK] gpt-4.1-nano-2025-04-14.csv\n",
      "Processed results/[HAERAE] Phi-4.csv\n",
      "Processed results/[HAERAE] gpt-4o-mini-240718.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4o-240513.csv\n",
      "Processed results/[KMMLU] gpt-4o-mini-240718-5shot.csv\n",
      "Processed results/[HAERAE] Phi-3-5-Moe-instruct.csv\n",
      "Processed results/[CLIcK] gpt-4o-mini-240718.csv\n",
      "Processed results/[CLIcK] Phi-3-5-MoE-instruct.csv\n",
      "Processed results/[HAERAE] gpt-4-turbo-240409.csv\n",
      "Processed results/[CLIcK] llama-3-1-8b-instruct.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4o-mini-240718-5shot.csv\n",
      "Processed results/[KMMLU] llama-3-1-8b-instruct-5shot.csv\n",
      "Processed results/[KMMLU-HARD] gpt-5-nano-2025-08-08-0shot.csv\n",
      "Processed results/[KMMLU-HARD] gpt-35-turbo-230613-5shot.csv\n",
      "Processed results/[HAERAE] gpt-5.1-chat-2025-11-13.csv\n",
      "Processed results/[KMMLU-HARD] gpt-5-chat-2025-08-08-0shot.csv\n",
      "Processed results/[KMMLU] Phi-3-mini-128k-June.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4-turbo-240409.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-5-mini-instruct.csv\n",
      "Processed results/[KMMLU-HARD] gpt-5.1-chat-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU] gpt-51-medium-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU-HARD] gpt-5-mini-2025-08-08-0shot.csv\n",
      "Processed results/[KMMLU] gpt-35-turbo-230613.csv\n",
      "Processed results/[HAERAE] gpt-5-nano-2025-08-08.csv\n",
      "Processed results/[KMMLU] gpt-4o-240513-5shot.csv\n",
      "Processed results/[KMMLU] gpt-5.1-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU] llama-3-1-8b-instruct.csv\n",
      "Processed results/[CLIcK] gpt-4o-240513.csv\n",
      "Processed results/[KMMLU] gpt-4o-mini-240718.csv\n",
      "Processed results/[KMMLU] gpt-4o-240513.csv\n",
      "Processed results/[HAERAE] gpt-51-medium-2025-11-13.csv\n",
      "Processed results/[HAERAE] gpt-4o-240513.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4o-mini-240718.csv\n",
      "Processed results/[CLIcK] gpt-35-turbo-230613.csv\n",
      "Processed results/[HAERAE] gpt-5-mini-2025-08-08.csv\n",
      "Processed results/[CLIcK] gpt-5-mini-2025-08-08.csv\n",
      "Processed results/[KMMLU] Phi-4-5shot.csv\n",
      "Processed results/[CLIcK] Phi-4.csv\n",
      "Processed results/[HAERAE] gpt-5.1-2025-11-13.csv\n",
      "Processed results/[HAERAE] gpt-4.1-nano-2025-04-14.csv\n",
      "Processed results/[KMMLU-HARD] gpt-35-turbo-230613.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-5-mini-instruct-5shot.csv\n",
      "Processed results/[CLIcK] Phi-3-5-mini-instruct.csv\n",
      "Processed results/[KMMLU] gpt-4.1-nano-2025-04-14.csv\n",
      "Processed results/[CLIcK] gpt-5.1-2025-11-13.csv\n",
      "Processed results/[KMMLU-HARD] Phi-4-5shot.csv\n",
      "Processed results/[KMMLU] Phi-3-5-mini-instruct-5shot.csv\n",
      "Processed results/[CLIcK] gpt-4.1-mini-2025-04-14.csv\n",
      "Processed results/[KMMLU-HARD] llama-3-1-8b-instruct-5shot.csv\n",
      "Processed results/[KMMLU] gpt-4-turbo-240409.csv\n",
      "Processed results/[CLIcK] gpt-4-turbo-240409.csv\n",
      "Processed results/[KMMLU-HARD] gpt-51-medium-2025-11-13-0shot.csv\n",
      "Processed results/[KMMLU] gpt-4-turbo-240409-5shot.csv\n",
      "Processed results/[CLIcK] gpt-5-nano-2025-08-08.csv\n",
      "Processed results/[CLIcK] gpt-5-chat-2025-08-08.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4.1-mini-2025-04-14.csv\n",
      "Processed results/[CLIcK] gpt-oss-120b-2025-08-05.csv\n",
      "Processed results/[CLIcK] gpt-51-medium-2025-11-13.csv\n",
      "Processed results/[HAERAE] Phi-3-mini-128k-June.csv\n",
      "Processed results/[HAERAE] gpt-5-chat-2025-08-08.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-mini-128k-June-5shot.csv\n",
      "Processed results/[KMMLU-HARD] Phi-3-5-MoE-instruct-5shot.csv\n",
      "Processed results/[KMMLU] gpt-5-nano-2025-08-08-0shot.csv\n",
      "Processed results/[HAERAE] gpt-4.1-mini-2025-04-14.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4.1-2025-04-14.csv\n",
      "Processed results/[KMMLU] gpt-35-turbo-230613-5shot.csv\n",
      "Processed results/[KMMLU-HARD] llama-3-1-8b-instruct.csv\n",
      "Processed results/[KMMLU-HARD] gpt-4.1-nano-2025-04-14.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from util.evaluate_helper import get_experiments_md\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_single_alphabet_answer(row):\n",
    "    pred = row[\"pred\"]\n",
    "    response = row[\"response\"]\n",
    "\n",
    "    if (\n",
    "        isinstance(pred, float)\n",
    "        and np.isnan(pred)\n",
    "        or (isinstance(pred, str) and len(pred.strip()) == 0)\n",
    "    ):\n",
    "        # response도 문자열인지 확인\n",
    "        if isinstance(response, str) and len(response.strip()) > 0:\n",
    "            match = re.search(r\"정답(?: \\(Answer\\))?: (\\w)\", response)\n",
    "            return match.group(1) if match else None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return pred\n",
    "\n",
    "csv_files = glob.glob(\"results/*.csv\")\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            print(f\"Skipped empty file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        df[\"pred\"] = df.apply(extract_single_alphabet_answer, axis=1)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Processed {file_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Skipped empty file (EmptyDataError): {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf64bdf-36ed-4e96-bdc5-59f70424d338",
   "metadata": {},
   "source": [
    "## CLIcK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a1e6299-99b6-4574-9502-9d9f22b8ee78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 80 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1796 valid responses\n",
      "Warning: Found 50 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1826 valid responses\n",
      "Warning: Found 60 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1816 valid responses\n",
      "Warning: Found 60 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1816 valid responses\n",
      "Warning: Found 20 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1856 valid responses\n",
      "Warning: Found 30 FAILED responses out of 1876 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1846 valid responses\n"
     ]
    }
   ],
   "source": [
    "dataset = \"CLIcK\"\n",
    "csv_path_dict = {\n",
    "    \"GPT-5.1-medium\": f\"results/[{dataset}] gpt-51-medium-2025-11-13.csv\",\n",
    "    \"GPT-5.1\": f\"results/[{dataset}] gpt-5.1-2025-11-13.csv\",\n",
    "    \"GPT-5.1-chat\": f\"results/[{dataset}] gpt-5.1-chat-2025-11-13.csv\",\n",
    "    \"GPT-5-chat\": f\"results/[{dataset}] gpt-5-chat-2025-08-08.csv\",\n",
    "    \"GPT-5-mini\": f\"results/[{dataset}] gpt-5-mini-2025-08-08.csv\",\n",
    "    \"GPT-5-nano\": f\"results/[{dataset}] gpt-5-nano-2025-08-08.csv\",\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-MoE-instruct.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613.csv\",\n",
    "}\n",
    "click_md = get_experiments_md(dataset, csv_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65d154-7d6e-444c-8e16-daf0ac22dfad",
   "metadata": {},
   "source": [
    "## HAERAE 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae55484c-834e-4614-9069-d7239c4bd9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 40 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1498 valid responses\n",
      "Warning: Found 20 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1518 valid responses\n",
      "Warning: Found 30 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1508 valid responses\n",
      "Warning: Found 90 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1448 valid responses\n",
      "Warning: Found 30 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1508 valid responses\n",
      "Warning: Found 10 FAILED responses out of 1538 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 1528 valid responses\n"
     ]
    }
   ],
   "source": [
    "dataset = \"HAERAE\"\n",
    "csv_path_dict = {\n",
    "    \"GPT-5.1-medium\": f\"results/[{dataset}] gpt-51-medium-2025-11-13.csv\",\n",
    "    \"GPT-5.1\": f\"results/[{dataset}] gpt-5.1-2025-11-13.csv\",\n",
    "    \"GPT-5.1-chat\": f\"results/[{dataset}] gpt-5.1-chat-2025-11-13.csv\",\n",
    "    \"GPT-5-chat\": f\"results/[{dataset}] gpt-5-chat-2025-08-08.csv\",\n",
    "    \"GPT-5-mini\": f\"results/[{dataset}] gpt-5-mini-2025-08-08.csv\",\n",
    "    \"GPT-5-nano\": f\"results/[{dataset}] gpt-5-nano-2025-08-08.csv\",\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-Moe-instruct.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613.csv\",\n",
    "}\n",
    "haerae_md = get_experiments_md(dataset, csv_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0a624-7fad-4ada-b998-db0cc77fdde4",
   "metadata": {},
   "source": [
    "## KMMLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897efbe",
   "metadata": {},
   "source": [
    "### zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf67094-1544-4655-aa85-824ccd783a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 1320 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 33710 valid responses\n",
      "Warning: Found 700 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 34330 valid responses\n",
      "Warning: Found 930 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 34100 valid responses\n",
      "Warning: Found 610 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 34420 valid responses\n",
      "Warning: Found 990 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 34040 valid responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 520 FAILED responses out of 35030 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 34510 valid responses\n"
     ]
    }
   ],
   "source": [
    "dataset = \"KMMLU\"\n",
    "csv_path_dict = {\n",
    "    \"GPT-5.1-medium\": f\"results/[{dataset}] gpt-51-medium-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5.1\": f\"results/[{dataset}] gpt-5.1-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5.1-chat\": f\"results/[{dataset}] gpt-5.1-chat-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5-chat\": f\"results/[{dataset}] gpt-5-chat-2025-08-08-0shot.csv\",\n",
    "    \"GPT-5-mini\": f\"results/[{dataset}] gpt-5-mini-2025-08-08-0shot.csv\",\n",
    "    \"GPT-5-nano\": f\"results/[{dataset}] gpt-5-nano-2025-08-08-0shot.csv\",\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4-0shot.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-MoE-instruct.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613.csv\",\n",
    "}\n",
    "kmmlu_md = get_experiments_md(dataset, csv_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64678ebe",
   "metadata": {},
   "source": [
    "### 5-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e914b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/[KMMLU] gpt-5.1-2025-11-13.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      2\u001b[39m postfix = \u001b[33m\"\u001b[39m\u001b[33m5shot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m csv_path_dict = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGPT-5.1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] gpt-5.1-2025-11-13.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGPT-5.1-chat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] gpt-5.1-chat-2025-11-13.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGPT-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] gpt-35-turbo-230613-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpostfix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m kmmlu_5shot_md = \u001b[43mget_experiments_md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostfix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/util/evaluate_helper.py:227\u001b[39m, in \u001b[36mget_experiments_md\u001b[39m\u001b[34m(dataset, csv_path_dict, postfix)\u001b[39m\n\u001b[32m    224\u001b[39m supercategory_acc = [\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(csv_path_dict)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, (k, csv_path) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(csv_path_dict.items()):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     overall_acc[idx], category_acc[idx], supercategory_acc[idx] = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_path_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m postfix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    232\u001b[39m     title = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/util/evaluate_helper.py:39\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(csv_path, dataset, verbose)\u001b[39m\n\u001b[32m     34\u001b[39m valid_datasets = [\u001b[33m\"\u001b[39m\u001b[33mCLIcK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKMMLU\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKMMLU-HARD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHAERAE\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     36\u001b[39m     dataset \u001b[38;5;129;01min\u001b[39;00m valid_datasets\n\u001b[32m     37\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m value. Please choose from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m result = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# FAILED 응답 필터링 및 로깅\u001b[39;00m\n\u001b[32m     42\u001b[39m original_count = \u001b[38;5;28mlen\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/afh/code/evaluate-llm-on-korean-dataset/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/[KMMLU] gpt-5.1-2025-11-13.csv'"
     ]
    }
   ],
   "source": [
    "dataset = \"KMMLU\"\n",
    "postfix = \"5shot\"\n",
    "\n",
    "csv_path_dict = {\n",
    "    \"GPT-5.1\": f\"results/[{dataset}] gpt-5.1-2025-11-13.csv\",\n",
    "    \"GPT-5.1-chat\": f\"results/[{dataset}] gpt-5.1-chat-2025-11-13.csv\",\n",
    "    \"GPT-5-chat\": f\"results/[{dataset}] gpt-5-chat-2025-08-08-5shot.csv\",\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4-{postfix}.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-MoE-instruct-{postfix}.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct-{postfix}.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June-{postfix}.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct-{postfix}.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513-{postfix}.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718-{postfix}.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409-{postfix}.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613-{postfix}.csv\",\n",
    "}\n",
    "kmmlu_5shot_md = get_experiments_md(dataset, csv_path_dict, postfix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97636dc7-0806-4e2a-ba15-7d84c4bc9a6e",
   "metadata": {},
   "source": [
    "## KMMLU-HARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646604f",
   "metadata": {},
   "source": [
    "### zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f366b7da-b91a-4f6b-83b8-879b45443827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 80 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4024 valid responses\n",
      "Warning: Found 40 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4064 valid responses\n",
      "Warning: Found 90 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4014 valid responses\n",
      "Warning: Found 80 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4024 valid responses\n",
      "Warning: Found 60 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4044 valid responses\n",
      "Warning: Found 40 FAILED responses out of 4104 total responses\n",
      "Excluding FAILED responses from accuracy calculation\n",
      "Evaluating on 4064 valid responses\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "csv_path = \"results/[KMMLU-HARD] Phi-4-0shot.csv\"\n",
    "if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "    print(f\"File {csv_path} does not exist or is empty.\")\n",
    "else:\n",
    "    result = pd.read_csv(csv_path)\n",
    "\n",
    "dataset = \"KMMLU-HARD\"\n",
    "csv_path_dict = {\n",
    "    \"GPT-5.1-medium\": f\"results/[{dataset}] gpt-51-medium-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5.1\": f\"results/[{dataset}] gpt-5.1-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5.1-chat\": f\"results/[{dataset}] gpt-5.1-chat-2025-11-13-0shot.csv\",\n",
    "    \"GPT-5-chat\": f\"results/[{dataset}] gpt-5-chat-2025-08-08-0shot.csv\",\n",
    "    \"GPT-5-mini\": f\"results/[{dataset}] gpt-5-mini-2025-08-08-0shot.csv\",\n",
    "    \"GPT-5-nano\": f\"results/[{dataset}] gpt-5-nano-2025-08-08-0shot.csv\",\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4-0shot.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-MoE-instruct.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613.csv\",\n",
    "}\n",
    "kmmlu_hard_md = get_experiments_md(dataset, csv_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30428a3a",
   "metadata": {},
   "source": [
    "### 5-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d0ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KMMLU-HARD\"\n",
    "postfix = \"5shot\"\n",
    "\n",
    "csv_path_dict = {\n",
    "    \"Phi-4\": f\"results/[{dataset}] Phi-4-{postfix}.csv\",\n",
    "    \"Phi-3.5-MoE-instruct\": f\"results/[{dataset}] Phi-3-5-MoE-instruct-{postfix}.csv\",\n",
    "    \"Phi-3.5-mini-instruct\": f\"results/[{dataset}] Phi-3-5-mini-instruct-{postfix}.csv\",\n",
    "    \"Phi-3-mini-128k-instruct-June\": f\"results/[{dataset}] Phi-3-mini-128k-June-{postfix}.csv\",\n",
    "    \"Llama-3.1-8B-Instruct\": f\"results/[{dataset}] llama-3-1-8b-instruct-{postfix}.csv\",\n",
    "    \"GPT-4o\": f\"results/[{dataset}] gpt-4o-240513-{postfix}.csv\",\n",
    "    \"GPT-4o-mini\": f\"results/[{dataset}] gpt-4o-mini-240718-{postfix}.csv\",\n",
    "    \"GPT-4-turbo\": f\"results/[{dataset}] gpt-4-turbo-240409-{postfix}.csv\",\n",
    "    \"GPT-3.5-turbo\": f\"results/[{dataset}] gpt-35-turbo-230613-{postfix}.csv\",\n",
    "}\n",
    "kmmlu_hard_5shot_md = get_experiments_md(dataset, csv_path_dict, postfix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86a08e",
   "metadata": {},
   "source": [
    "## Save to the Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "066cf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DETAILED_RESULTS.md\", \"w\") as f:\n",
    "    f.write(\"## Detailed Results\\n\\n\")\n",
    "    f.write(click_md)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(haerae_md)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(kmmlu_md)\n",
    "    f.write(\"\\n\\n\")\n",
    "    # f.write(kmmlu_5shot_md)\n",
    "    # f.write(\"\\n\\n\")\n",
    "    f.write(kmmlu_hard_md)\n",
    "    f.write(\"\\n\\n\")\n",
    "    # f.write(kmmlu_hard_5shot_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0006bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluate-llm-on-korean-dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
