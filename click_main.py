import os
import json
import time
import argparse
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp

import openai
from openai import RateLimitError
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from datasets import load_dataset

from prompts import TYPE_1, TYPE_2, TYPE_3, TYPE_4
from util.custom_parser import MultipleChoicesFiveParser

from util.common_helper import (
    str2bool,
    format_timespan,
    get_prompt_template,
    get_llm_client,
)

from logger import logger


def get_prompt(x) -> str:
    num_choices = len(x["choices"])
    if num_choices == 4:
        if x["paragraph"] != "":  # Use Type 1 Prompt
            return TYPE_1.format(
                CONTEXT=x["paragraph"],
                QUESTION=x["question"],
                A=x["choices"][0],
                B=x["choices"][1],
                C=x["choices"][2],
                D=x["choices"][3],
            )
        else:
            return TYPE_2.format(
                QUESTION=x["question"],
                A=x["choices"][0],
                B=x["choices"][1],
                C=x["choices"][2],
                D=x["choices"][3],
            )
    elif num_choices == 5:
        if x["paragraph"] != "":
            return TYPE_3.format(
                CONTEXT=x["paragraph"],
                QUESTION=x["question"],
                A=x["choices"][0],
                B=x["choices"][1],
                C=x["choices"][2],
                D=x["choices"][3],
                E=x["choices"][4],
            )
        else:
            return TYPE_4.format(
                QUESTION=x["question"],
                A=x["choices"][0],
                B=x["choices"][1],
                C=x["choices"][2],
                D=x["choices"][3],
                E=x["choices"][4],
            )
    else:
        raise ValueError(f"Invalid number of choices: {num_choices} (ID: {x['id']})")


def get_answer(x) -> str:
    answer_idx = [xx.strip() for xx in x["choices"]].index(x["answer"].strip())
    if answer_idx == -1:
        raise ValueError(f"Answer not found in choices: {x['answer']} (ID: {x['id']})")
    return chr(0x41 + answer_idx)  # answer_idx = 0 -> answer = "A"


def get_category_from_id(item_id):
    """IDì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    with open("id_to_category.json", "r") as json_file:
        id_to_category = json.load(json_file)
    return id_to_category.get(str(item_id), "Unknown")


def load_existing_results(csv_path):
    """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ"""
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            if not df.empty:
                logger.info(f"Found existing results: {len(df)} records in {csv_path}")
                return df
        except Exception as e:
            logger.warning(f"Error loading existing results: {e}")
    return pd.DataFrame()


def _save_results_safely(responses, csv_path):
    """ì‹¤ì œ ì €ì¥ ë¡œì§"""
    df_new = pd.DataFrame(responses)
    
    # íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    abs_csv_path = os.path.abspath(csv_path)
    
    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³  í•©ì¹˜ê¸°
    if os.path.exists(csv_path):
        df_existing = pd.read_csv(csv_path)
        # ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(subset=['id'], keep='last')
        
        logger.info(f"ğŸ“ Updated existing file: {abs_csv_path}")
        logger.info(f"   - Added {len(df_new)} new records")
        logger.info(f"   - Total records after merge: {len(df_combined)}")
    else:
        df_combined = df_new
        logger.info(f"ğŸ“ Created new file: {abs_csv_path}")
        logger.info(f"   - Initial records: {len(df_combined)}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
    os.makedirs(os.path.dirname(abs_csv_path), exist_ok=True)
    
    # íŒŒì¼ ì €ì¥
    df_combined.to_csv(csv_path, index=False)
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(csv_path)
    file_size_mb = file_size / (1024 * 1024)
    
    logger.info(f"âœ… Successfully saved CSV file:")
    logger.info(f"   - File path: {abs_csv_path}")
    logger.info(f"   - File size: {file_size:,} bytes ({file_size_mb:.2f} MB)")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ë¡œê¹… (CLIcKì˜ ê²½ìš°)
    if 'id' in df_combined.columns:
        try:
            with open("id_to_category.json", "r") as json_file:
                id_to_category = json.load(json_file)
            df_combined["category"] = df_combined["id"].astype(str).map(id_to_category)
            category_counts = df_combined['category'].value_counts()
            logger.info(f"   - Categories saved: {list(category_counts.index)}")
            logger.info(f"   - Records per category: {dict(category_counts)}")
        except Exception as e:
            logger.warning(f"Could not analyze category distribution: {e}")


def save_results_incremental(responses, csv_path, lock=None):
    """ê²°ê³¼ë¥¼ ì ì§„ì ìœ¼ë¡œ ì €ì¥ (ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „)"""
    try:
        if lock:
            with lock:
                _save_results_safely(responses, csv_path)
        else:
            _save_results_safely(responses, csv_path)
    except Exception as e:
        logger.error(f"âŒ Error saving results to {os.path.abspath(csv_path)}: {e}")
        raise


def get_completed_categories(csv_path, min_records=10):
    """ì™„ë£Œëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            if not df.empty:
                # IDë¥¼ í†µí•´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
                with open("id_to_category.json", "r") as json_file:
                    id_to_category = json.load(json_file)
                
                df["category"] = df["id"].astype(str).map(id_to_category)
                category_counts = df['category'].value_counts()
                completed = []
                for category, count in category_counts.items():
                    logger.info(f"Found category {category} with {count} records")
                    if count >= min_records:
                        completed.append(category)
                logger.info(f"Completed categories: {completed}")
                return completed
        except Exception as e:
            logger.warning(f"Error reading completed categories: {e}")
    return []


def get_category_data_ranges(click_ds):
    """ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë²”ìœ„ ê³„ì‚°"""
    with open("id_to_category.json", "r") as json_file:
        id_to_category = json.load(json_file)
    
    category_ranges = {}
    current_category = None
    start_idx = 0
    
    for idx, item in enumerate(click_ds):
        item_category = id_to_category.get(str(item["id"]), "Unknown")
        
        if current_category != item_category:
            if current_category is not None:
                category_ranges[current_category] = (start_idx, idx)
            current_category = item_category
            start_idx = idx
    
    # ë§ˆì§€ë§‰ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    if current_category is not None:
        category_ranges[current_category] = (start_idx, len(click_ds))
    
    return category_ranges


def process_batch_streaming(batch_data, model_config, template_type="basic"):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë³„ë„ì˜ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        llm, _ = get_llm_client(
            model_config['provider'], 
            model_config.get('hf_model_id', 'microsoft/Phi-3.5-mini-instruct'),
            model_config['temperature'], 
            model_config['max_tokens'], 
            model_config['max_retries'],
            model_config.get('wait_time', 1.0)
        )
        
        prompt_template = get_prompt_template(template_type, model_config['provider'])
        chain = prompt_template | llm | MultipleChoicesFiveParser()
        
        results = []
        batch_size = model_config['batch_size']
        max_retries = model_config['max_retries']
        
        for i in range(0, len(batch_data), batch_size):
            mini_batch = batch_data[i:i + batch_size]
            retries = 0
            
            while retries <= max_retries:
                try:
                    # Handle LangChain chain
                    preds = chain.batch(mini_batch, {"max_concurrency": batch_size})
                    
                    for qna, pred in zip(mini_batch, preds):
                        results.append({
                            "id": qna["id"],
                            "trial": 0,
                            "answer": qna["answer"],
                            "pred": pred[0],
                            "response": pred[1],
                        })
                    break
                    
                except RateLimitError as e:
                    delay = (retries + 1) * 30
                    logger.warning(f"Rate limit error, retrying in {delay} seconds...")
                    time.sleep(delay)
                    retries += 1
                    
                    if retries > max_retries:
                        logger.error(f"Max retries reached for batch")
                        for qna in mini_batch:
                            results.append({
                                "id": qna["id"],
                                "trial": 0,
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": "RATE_LIMIT_ERROR",
                            })
                        break
                        
                except openai.BadRequestError as e:
                    logger.error(f"BadRequestError: {e}. Adding failed responses for this batch.")
                    logger.info(f"Question sample: {batch_data[i]['question'][:100]}..." if batch_data else "No question data")
                    # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    for qna in mini_batch:
                        results.append({
                            "id": qna["id"],
                            "trial": 0,
                            "answer": qna["answer"],
                            "pred": "FAILED",
                            "response": "BAD_REQUEST_ERROR",
                        })
                    break
                        
                except KeyError as e:
                    # í•µì‹¬ 'choices' KeyError ì²˜ë¦¬ ì¶”ê°€
                    if "'choices'" in str(e):
                        logger.warning(f"OpenAI API response missing 'choices' field - processing individually")
                        # ê°œë³„ ì²˜ë¦¬ë¡œ fallback
                        for qna in mini_batch:
                            try:
                                pred = chain.invoke(qna["question"])
                                results.append({
                                    "id": qna["id"],
                                    "trial": 0,
                                    "answer": qna["answer"],
                                    "pred": pred[0],
                                    "response": pred[1],
                                })
                            except Exception as individual_error:
                                logger.error(f"Individual processing failed: {individual_error}")
                                results.append({
                                    "id": qna["id"],
                                    "trial": 0,
                                    "answer": qna["answer"],
                                    "pred": "FAILED",
                                    "response": f"ERROR: {str(individual_error)}",
                                })
                        break
                    else:
                        # ë‹¤ë¥¸ KeyErrorëŠ” ì¼ë°˜ ì²˜ë¦¬
                        logger.error(f"KeyError in batch processing: {e}")
                        retries += 1
                        if retries > max_retries:
                            for qna in mini_batch:
                                results.append({
                                    "id": qna["id"],
                                    "trial": 0,
                                    "answer": qna["answer"],
                                    "pred": "FAILED",
                                    "response": f"KEYERROR: {str(e)}",
                                })
                            break
                        time.sleep(2 ** retries)
                        
                except Exception as e:
                    logger.error(f"Error processing batch: {e}")
                    retries += 1
                    if retries > max_retries:
                        for qna in mini_batch:
                            results.append({
                                "id": qna["id"],
                                "trial": 0,
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": f"ERROR: {str(e)}",
                            })
                        break
                    time.sleep(2 ** retries)
        
        return results
        
    except Exception as e:
        logger.error(f"Error in process_batch_streaming: {e}")
        return []


def process_category_streaming(category_info):
    """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
    category, data_range, click_ds, model_config, is_debug, num_debug_samples, template_type, csv_path = category_info
    
    logger.info(f"Processing category {category} in process {os.getpid()}")
    
    try:
        start_idx, end_idx = data_range
        category_ds = click_ds.select(range(start_idx, end_idx))
        
        if is_debug:
            category_ds = category_ds.select(range(min(num_debug_samples, len(category_ds))))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ í¬ê¸° ì„¤ì •
        chunk_size = model_config['batch_size'] * 10
        total_items = len(category_ds)
        category_responses = []
        
        logger.info(f"Processing {total_items} items for category {category} in chunks of {chunk_size}")
        
        # ì²­í¬ë³„ ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€
        total_chunks = (total_items + chunk_size - 1) // chunk_size
        with tqdm(total=total_chunks, desc=f"Processing {category}", position=0, leave=True) as pbar:
            # ì²­í¬ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            for start_chunk in range(0, total_items, chunk_size):
                end_chunk = min(start_chunk + chunk_size, total_items)
                chunk_ds = category_ds.select(range(start_chunk, end_chunk))
                
                # ì²­í¬ë¥¼ ë°°ì¹˜ ë°ì´í„°ë¡œ ë³€í™˜
                chunk_batch = [
                    {
                        "id": x["id"], 
                        "question": get_prompt(x), 
                        "answer": get_answer(x)
                    }
                    for x in chunk_ds
                ]
                
                # ì²­í¬ ì²˜ë¦¬
                chunk_results = process_batch_streaming(chunk_batch, model_config, template_type)
                category_responses.extend(chunk_results)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                pbar.update(1)
                pbar.set_postfix({
                    'items': f"{len(category_responses)}/{total_items}",
                    'chunk_size': len(chunk_results)
                })
                
                # ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                if len(category_responses) >= 1000:
                    save_results_incremental(category_responses, csv_path)
                    logger.info(f"Intermediate save for category {category}: {len(category_responses)} items")
                    category_responses = []
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥
        if category_responses:
            save_results_incremental(category_responses, csv_path)
        
        logger.info(f"Completed category {category}")
        return category, "completed"
        
    except Exception as e:
        logger.error(f"Error processing category {category}: {e}")
        return category, f"error: {str(e)}"


def benchmark_multiprocess(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    is_debug = args.is_debug
    
    # ëª¨ë¸ ì„¤ì •
    model_name = os.getenv("MODEL_NAME", "gpt-5-mini")
    model_version = os.getenv("MODEL_VERSION", "2025-08-08")
    
    model_config = {
        'provider': args.model_provider,
        'hf_model_id': args.hf_model_id,
        'batch_size': args.batch_size,
        'max_tokens': args.max_tokens,
        'temperature': args.temperature,
        'max_retries': args.max_retries,
        'wait_time': args.wait_time,
    }
    
    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    os.makedirs("results", exist_ok=True)
    csv_path = f"results/[CLIcK] {model_name}-{model_version}.csv"
    abs_csv_path = os.path.abspath(csv_path)
    
    logger.info(f"ğŸ¯ Target output file: {abs_csv_path}")
    
    # ê¸°ì¡´ íŒŒì¼ ìƒíƒœ í™•ì¸
    if os.path.exists(csv_path):
        file_size = os.path.getsize(csv_path)
        existing_df = pd.read_csv(csv_path)
        logger.info(f"ğŸ“‹ Found existing file with {len(existing_df)} records ({file_size:,} bytes)")
    else:
        logger.info(f"ğŸ“‹ No existing file found - will create new file")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    click_ds = load_dataset("EunsuKim/CLIcK")["train"]
    
    if is_debug:
        click_ds = click_ds.select(range(args.num_debug_samples))
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë²”ìœ„ ê³„ì‚°
    category_ranges = get_category_data_ranges(click_ds)
    
    # ëª¨ë“  CLIcK ì¹´í…Œê³ ë¦¬ ëª©ë¡
    all_click_categories = list(category_ranges.keys())
    
    # ì‹¤í–‰í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
    if args.categories:
        # ì‚¬ìš©ìê°€ ì§€ì •í•œ ì¹´í…Œê³ ë¦¬ë“¤ ê²€ì¦
        invalid_categories = [c for c in args.categories if c not in all_click_categories]
        if invalid_categories:
            logger.error(f"Invalid categories specified: {invalid_categories}")
            logger.error(f"Available categories: {all_click_categories}")
            return
        selected_categories = args.categories
        logger.info(f"ğŸ¯ Processing user-specified categories: {selected_categories}")
        
        # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ë“¤ë§Œ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ category_ranges ìƒì„±
        filtered_category_ranges = {cat: category_ranges[cat] for cat in selected_categories}
        category_ranges = filtered_category_ranges
    else:
        selected_categories = all_click_categories
        logger.info(f"ğŸ¯ Processing all categories: {selected_categories}")
    
    # ì™„ë£Œëœ ì¹´í…Œê³ ë¦¬ í™•ì¸
    completed_categories = get_completed_categories(csv_path)
    
    # ë‚¨ì€ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (start_category ë¡œì§ ì œê±°)
    remaining_categories = [c for c in selected_categories if c not in completed_categories]
    
    if not remaining_categories:
        logger.info("All specified categories already completed!")
        return
    
    logger.info(f"Processing {len(remaining_categories)} remaining categories: {remaining_categories}")
    logger.info(f"Using multiprocessing with {args.max_workers} workers")
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‘ì—… ì¤€ë¹„
    category_tasks = [
        (category, category_ranges[category], click_ds, model_config, 
         is_debug, args.num_debug_samples, args.template_type, csv_path)
        for category in remaining_categories
    ]
    
    start_time = time.time()
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰ - tqdm ê°œì„ 
    with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
        future_to_category = {
            executor.submit(process_category_streaming, task): task[0] 
            for task in category_tasks
        }
        
        completed_count = 0
        # ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 
        with tqdm(total=len(remaining_categories), desc="Categories", position=1, leave=True) as category_pbar:
            for future in future_to_category:
                category = future_to_category[future]
                try:
                    result_category, status = future.result()
                    completed_count += 1
                    logger.info(f"Category {result_category} completed: {status} ({completed_count}/{len(remaining_categories)})")
                    
                    # ì¹´í…Œê³ ë¦¬ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    category_pbar.update(1)
                    category_pbar.set_postfix({
                        'current': result_category,
                        'status': status
                    })
                    
                except Exception as e:
                    logger.error(f"Category {category} failed with exception: {e}")
                    completed_count += 1
                    category_pbar.update(1)
                    category_pbar.set_postfix({
                        'current': category,
                        'status': 'failed'
                    })

    end_time = time.time()
    total_time = format_timespan(end_time - start_time)
    
    logger.info(f"====== [DONE] All specified categories processed in {total_time} =====")
    
    # ìµœì¢… íŒŒì¼ ìƒíƒœ í™•ì¸
    if os.path.exists(csv_path):
        final_df = pd.read_csv(csv_path)
        final_file_size = os.path.getsize(csv_path)
        logger.info(f"ğŸ Final output file status:")
        logger.info(f"   - Path: {abs_csv_path}")
        logger.info(f"   - Records: {len(final_df)}")
        logger.info(f"   - Size: {final_file_size:,} bytes ({final_file_size/(1024*1024):.2f} MB)")
    else:
        logger.error(f"âŒ Final output file not found: {abs_csv_path}")
    
    # ìµœì¢… í‰ê°€
    logger.info(f"====== [START] Final Evaluation - CSV_PATH: {csv_path} =====")
    evaluate(csv_path)
    logger.info(f"====== [END] Evaluation completed =====")


def benchmark_sequential(args):
    """ê¸°ì¡´ ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹"""
    is_debug = args.is_debug
    max_retries = args.max_retries
    delay_increment = 30

    num_debug_samples = args.num_debug_samples
    batch_size = args.batch_size
    max_tokens = args.max_tokens
    temperature = args.temperature
    llm, model_name = get_llm_client(
        args.model_provider, args.hf_model_id, temperature, max_tokens, max_retries, args.wait_time
    )
    model_version = (
        os.getenv("MODEL_VERSION")
        if args.model_provider == "azureopenai"
        else None
    )

    click_ds = load_dataset("EunsuKim/CLIcK")["train"]

    if is_debug:
        click_ds = click_ds.select(range(num_debug_samples))

    all_batch = [
        {"id": x["id"], "question": get_prompt(x), "answer": get_answer(x)}
        for x in tqdm(click_ds)
    ]
    responses = []
    prompt_template = get_prompt_template(args.template_type, args.model_provider)
    chain = prompt_template | llm | MultipleChoicesFiveParser()

    logger.info(f"====== [START] Generate answers to questions given by LLM. =====")
    logger.info(
        f"====== deployment name: {model_name}, model version: {model_version} ====="
    )
    t0 = time.time()

    with tqdm(total=len(all_batch), desc="Processing Answers") as pbar:

        for i in range(0, len(all_batch), batch_size):
            mini_batch = all_batch[i : i + batch_size]
            retries = 0

            while retries <= max_retries:
                try:
                    preds = chain.batch(mini_batch, {"max_concurrency": batch_size})
                    # If no exception, add questions and answers to all_answers
                    
                    for qna, pred in zip(mini_batch, preds):
                        responses.append(
                            {
                                "id": qna["id"],
                                "trial": 0,
                                "answer": qna["answer"],
                                "pred": pred[0],
                                "response": pred[1],
                            }
                        )
                    break  # Exit the retry loop once successful
                except RateLimitError as rate_limit_error:
                    delay = (retries + 1) * delay_increment
                    logger.warning(
                        f"{rate_limit_error}. Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                    retries += 1

                    if retries > max_retries:
                        logger.error(
                            f"Max retries reached this batch. Adding failed responses for this batch."
                        )
                        # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                        for qna in mini_batch:
                            responses.append(
                                {
                                    "id": qna["id"],
                                    "trial": 0,
                                    "answer": qna["answer"],
                                    "pred": "FAILED",
                                    "response": "RATE_LIMIT_ERROR",
                                }
                            )
                        break
                except openai.BadRequestError as e:
                    logger.error(f"BadRequestError: {e}. Adding failed responses for this batch.")
                    logger.info(f"Question sample: {mini_batch[0]['question'][:100]}...")
                    # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    for qna in mini_batch:
                        responses.append(
                            {
                                "id": qna["id"],
                                "trial": 0,
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": "BAD_REQUEST_ERROR",
                            }
                        )
                    break
                except Exception as e:
                    logger.error(f"Error in process_inputs: {e}. Adding failed responses for this batch.")
                    # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    for qna in mini_batch:
                        responses.append(
                            {
                                "id": qna["id"],
                                "trial": 0,
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": f"ERROR: {str(e)}",
                            }
                        )
                    break

            pbar.set_postfix(
                {
                    "current_batch": f"{i//batch_size + 1}/{(len(all_batch) + (batch_size-1))//batch_size}"
                }
            )
            pbar.update(len(mini_batch))

    t1 = time.time()
    timespan = format_timespan(t1 - t0)
    logger.info(f"===== [DONE] Generating Answer dataset took {timespan}")

    if not responses:
        logger.error("No successful responses were generated. Skipping evaluation.")
        return

    df = pd.DataFrame(responses)
    os.makedirs("results", exist_ok=True)
    csv_path = f"results/[CLIcK] {model_name}-{model_version}.csv"
    abs_csv_path = os.path.abspath(csv_path)
    
    df.to_csv(csv_path, index=False)
    
    file_size = os.path.getsize(csv_path)
    logger.info(f"âœ… Successfully saved CSV file:")
    logger.info(f"   - File path: {abs_csv_path}")
    logger.info(f"   - Records: {len(df)}")
    logger.info(f"   - File size: {file_size:,} bytes ({file_size/(1024*1024):.2f} MB)")
    
    logger.info(f"====== [START] Evaluation start - CSV_PATH: {csv_path} =====")
    evaluate(csv_path)
    logger.info(f"====== [START] Evaluation end =====")


def evaluate(csv_path):
    abs_csv_path = os.path.abspath(csv_path)
    
    if not os.path.exists(csv_path):
        logger.error(f"âŒ CSV file does not exist: {abs_csv_path}")
        return
    
    logger.info(f"ğŸ“Š Starting evaluation of: {abs_csv_path}")
    
    result = pd.read_csv(csv_path)
    if result.empty:
        logger.error(f"âŒ CSV file is empty: {abs_csv_path}")
        return
    
    logger.info(f"ğŸ“Š Loaded {len(result)} records for evaluation")
    
    # FAILED ì‘ë‹µ í•„í„°ë§ ë° ë¡œê¹…
    original_count = len(result)
    failed_count = len(result[result["pred"] == "FAILED"])
    if failed_count > 0:
        logger.warning(f"Found {failed_count} FAILED responses out of {original_count} total responses")
        logger.info(f"Excluding FAILED responses from accuracy calculation")
        result = result[result["pred"] != "FAILED"]
        logger.info(f"Evaluating on {len(result)} valid responses")
    
    with open("id_to_category.json", "r") as json_file:
        id_to_category = json.load(json_file)

    result["category"] = result["id"].astype(str).map(id_to_category)
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ IDë“¤ í™•ì¸ ë° ì œê±°
    missing_ids = result[result["category"].isna()]["id"].unique()
    if len(missing_ids) > 0:
        logger.warning(f"Found IDs without category mapping: {missing_ids[:10]}...")
        logger.warning(f"Total missing IDs: {len(missing_ids)}")
        result = result.dropna(subset=["category"])
    
    result["correct"] = result["answer"] == result["pred"]
    result["category_big"] = result["category"].apply(
        lambda x: (
            "Culture"
            if x
            in [
                "Economy",
                "Geography",
                "History",
                "Law",
                "Politics",
                "Popular",
                "Society",
                "Tradition",
                "Pop Culture",
            ]
            else ("Language" if x in ["Functional", "Textual", "Grammar"] else "Other")
        )
    )

    category_avg = (
        result.groupby(["category_big", "category"])
        .agg(correct_mean=("correct", "mean"), correct_count=("correct", "size"))
        .reset_index()
    )
    print(category_avg)

    category_big_avg = (
        result.groupby("category_big")
        .agg(correct_mean=("correct", "mean"), correct_count=("correct", "size"))
        .reset_index()
    )
    print(category_big_avg)

    # ì „ì²´ í‰ê·  ê³„ì‚°
    overall_avg = result["correct"].mean()
    print(f"Overall Average: {overall_avg}")

    os.makedirs("evals", exist_ok=True)
    filename = csv_path.split("/")[-1].split(".")[0]
    
    eval_file1 = f"evals/{filename}-eval.csv"
    eval_file2 = f"evals/{filename}-eval-avg.csv"
    
    category_avg.to_csv(eval_file1, index=False)
    category_big_avg.to_csv(eval_file2, index=False)
    
    abs_eval_file1 = os.path.abspath(eval_file1)
    abs_eval_file2 = os.path.abspath(eval_file2)
    
    logger.info(f"âœ… Evaluation results saved:")
    logger.info(f"   - Detailed results: {abs_eval_file1}")
    logger.info(f"   - Summary results: {abs_eval_file2}")


if __name__ == "__main__":
    dotenv_path = os.getenv('DOTENV_PATH', '.env')
    load_dotenv(dotenv_path, override=True)
   
    parser = argparse.ArgumentParser(description="CLIcK Benchmark with Multiprocessing and Streaming")
    parser.add_argument("--is_debug", type=str2bool, default=True)
    parser.add_argument("--num_debug_samples", type=int, default=20)
    parser.add_argument("--model_provider", type=str, default="azureopenai")
    parser.add_argument("--hf_model_id", type=str, default="microsoft/Phi-3.5-MoE-instruct")
    parser.add_argument("--batch_size", type=int, default=10)
    parser.add_argument("--max_retries", type=int, default=2)
    parser.add_argument("--max_tokens", type=int, default=256)
    parser.add_argument("--temperature", type=float, default=0.01)
    parser.add_argument("--template_type", type=str, default="basic")
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë“¤ë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì¸ìˆ˜
    parser.add_argument(
        "--categories", 
        type=str, 
        nargs='*', 
        default=None, 
        help='Specific categories to process (e.g., --categories "Economy" "Geography")'
    )
    
    # ìƒˆë¡œìš´ ë©€í‹°í”„ë¡œì„¸ì‹± ê´€ë ¨ ì¸ìˆ˜
    parser.add_argument("--use_multiprocessing", type=str2bool, default=True, help="Enable multiprocessing")
    parser.add_argument("--max_workers", type=int, default=3, help="Maximum number of worker processes")
    parser.add_argument("--wait_time", type=float, default=5.0, help="Wait time between Bedrock requests to avoid throttling")

    args = parser.parse_args()
    valid_providers = ["azureopenai", "openai", "azureml", "azureai", "huggingface", "bedrock"]
    assert (
        args.model_provider in valid_providers
    ), f"Invalid 'model_provider' value. Please choose from {valid_providers}."

    valid_template_types = ["basic", "chat", "gpt5"]
    assert (
        args.template_type in valid_template_types
    ), f"Invalid 'template_type' value. Please choose from {valid_template_types}."

    # ì¹´í…Œê³ ë¦¬ ì¸ìˆ˜ ë¡œê¹…
    if args.categories:
        logger.info(f"ğŸ¯ User specified categories: {args.categories}")
    else:
        logger.info(f"ğŸ¯ Will process all available categories")

    logger.info(args)
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‹¤í–‰ ë°©ì‹ ì„ íƒ
    if args.use_multiprocessing and args.max_workers > 1:
        benchmark_multiprocess(args)
    else:
        benchmark_sequential(args)