import os
import json
import time
import random
import argparse
import asyncio
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError
from multiprocessing import Manager, Queue
import multiprocessing as mp

import openai
from openai import RateLimitError
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from datasets import Dataset, load_dataset

from prompts import TYPE_1, TYPE_2, TYPE_3, TYPE_4, TYPE_MMLU_FEW_SHOT
from util.common_helper import (
    str2bool,
    format_timespan,
    get_prompt_template,
    get_llm_client,
)
from logger import logger
from util.custom_parser import MultipleChoicesFourParser


def generate_few_shots_prompt(data):
    prompt = ""
    for i, row in enumerate(data):
        prompt += f"ì§ˆë¬¸ (Question): {row['question']}\n"
        prompt += f"ë³´ê¸° (Options)\nA: {row['A']}, B: {row['B']}, C: {row['C']}, D: {row['D']}\n"
        prompt += f"ì •ë‹µ (Answer): {row['answer']}\n\n"
    return prompt


def get_prompt(x, few_shots=None) -> str:
    if few_shots is None:
        return TYPE_2.format(
            QUESTION=x["question"],
            A=x["A"],
            B=x["B"],
            C=x["C"],
            D=x["D"],
        )
    else:
        return TYPE_MMLU_FEW_SHOT.format(
            FEW_SHOTS=few_shots,
            QUESTION=x["question"],
            A=x["A"],
            B=x["B"],
            C=x["C"],
            D=x["D"],
        )


def get_answer(x) -> str:
    return x["answer"].upper().strip()


def map_answer(answer):
    answer_mapping = {1: "A", 2: "B", 3: "C", 4: "D"}
    return answer_mapping[answer]


def convert_to_pascal_case(category):
    return "-".join(word.capitalize() for word in category.split("_"))


def load_existing_results(csv_path):
    """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ"""
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            if not df.empty:
                logger.info(f"Found existing results: {len(df)} records in {csv_path}")
                return df
        except Exception as e:
            logger.warning(f"Error loading existing results: {e}")
    return pd.DataFrame()


def _save_results_safely(responses, csv_path):
    """ì‹¤ì œ ì €ì¥ ë¡œì§"""
    df_new = pd.DataFrame(responses)
    
    # íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    abs_csv_path = os.path.abspath(csv_path)
    
    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³  í•©ì¹˜ê¸°
    if os.path.exists(csv_path):
        df_existing = pd.read_csv(csv_path)
        # ê¸°ì¡´ ë°ì´í„°ì—ì„œ í˜„ì¬ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì œê±° (ë®ì–´ì“°ê¸° ìœ„í•´)
        current_category = df_new['category'].iloc[0] if not df_new.empty else None
        if current_category:
            df_existing = df_existing[df_existing['category'] != current_category]
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        
        logger.info(f"ğŸ“ Updated existing file: {abs_csv_path}")
        logger.info(f"   - Added {len(df_new)} new records for category: {current_category}")
        logger.info(f"   - Total records after merge: {len(df_combined)}")
    else:
        df_combined = df_new
        current_category = df_new['category'].iloc[0] if not df_new.empty else "Unknown"
        logger.info(f"ğŸ“ Created new file: {abs_csv_path}")
        logger.info(f"   - Initial records for category {current_category}: {len(df_combined)}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
    os.makedirs(os.path.dirname(abs_csv_path), exist_ok=True)
    
    # íŒŒì¼ ì €ì¥
    df_combined.to_csv(csv_path, index=False)
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(csv_path)
    file_size_mb = file_size / (1024 * 1024)
    
    logger.info(f"âœ… Successfully saved CSV file:")
    logger.info(f"   - File path: {abs_csv_path}")
    logger.info(f"   - File size: {file_size:,} bytes ({file_size_mb:.2f} MB)")
    logger.info(f"   - Total records: {len(df_combined)}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ë¡œê¹… (KMMLUì˜ ê²½ìš°)
    try:
        category_counts = df_combined['category'].value_counts()
        logger.info(f"   - Categories saved: {list(category_counts.index)}")
        logger.info(f"   - Records per category: {dict(category_counts)}")
    except Exception as e:
        logger.warning(f"Could not analyze category distribution: {e}")


def save_results_incremental(responses, csv_path, lock=None):
    """ê²°ê³¼ë¥¼ ì ì§„ì ìœ¼ë¡œ ì €ì¥ (ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „)"""
    try:
        if lock:
            with lock:
                _save_results_safely(responses, csv_path)
        else:
            _save_results_safely(responses, csv_path)
    except Exception as e:
        logger.error(f"âŒ Error saving results to {os.path.abspath(csv_path)}: {e}")
        raise


def get_completed_categories(csv_path, min_records=10):
    """ì™„ë£Œëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            if not df.empty:
                category_counts = df['category'].value_counts()
                completed = []
                for category, count in category_counts.items():
                    logger.info(f"Found category {category} with {count} records")
                    if count >= min_records:
                        completed.append(category)
                logger.info(f"Completed categories: {completed}")
                return completed
        except Exception as e:
            logger.warning(f"Error reading completed categories: {e}")
    return []


def process_batch_streaming(batch_data, model_config, few_shots_prompt, template_type="basic"):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë³„ë„ì˜ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        llm, _ = get_llm_client(
            model_config['provider'], 
            model_config.get('hf_model_id', 'microsoft/Phi-3.5-mini-instruct'),
            model_config['temperature'], 
            model_config['max_tokens'], 
            model_config['max_retries'],
            model_config.get('wait_time', 1.0)
        )
        
        prompt_template = get_prompt_template(template_type)
        chain = prompt_template | llm | MultipleChoicesFourParser()
        
        results = []
        batch_size = model_config['batch_size']
        max_retries = model_config['max_retries']
        
        for i in range(0, len(batch_data), batch_size):
            mini_batch = batch_data[i:i + batch_size]
            retries = 0
            
            while retries <= max_retries:
                try:
                    preds = chain.batch(mini_batch, {"max_concurrency": batch_size})
                    
                    for qna, pred in zip(mini_batch, preds):
                        results.append({
                            "category": qna["category"],
                            "answer": qna["answer"],
                            "pred": pred[0],
                            "response": pred[1],
                        })
                    break
                    
                except RateLimitError as e:
                    delay = (retries + 1) * 30
                    logger.warning(f"Rate limit error, retrying in {delay} seconds...")
                    time.sleep(delay)
                    retries += 1
                    
                    if retries > max_retries:
                        logger.error(f"Max retries reached for batch")
                        for qna in mini_batch:
                            results.append({
                                "category": qna["category"],
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": "RATE_LIMIT_ERROR",
                            })
                        break
                        
                except openai.BadRequestError as e:
                    error_msg = str(e)
                    
                    # Content filtering ì—ëŸ¬ ì²˜ë¦¬
                    if "prompt_filter_results" in error_msg or "Response missing `choices` key" in error_msg:
                        logger.warning(f"Content filtering detected for batch - marking as FILTERED")
                        for qna in mini_batch:
                            results.append({
                                "category": qna["category"],
                                "answer": qna["answer"],
                                "pred": "FILTERED",
                                "response": "CONTENT_FILTER_ERROR",
                            })
                        break  # ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ë°°ì¹˜ë¡œ
                    else:
                        logger.error(f"BadRequestError: {e}. Adding failed responses for this batch.")
                        logger.info(f"Question sample: {batch_data[i]['question'][:100]}..." if batch_data else "No question data")
                        # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                        for qna in mini_batch:
                            results.append({
                                "category": qna["category"],
                                "answer": qna["answer"],
                                "pred": "FAILED",
                                "response": "BAD_REQUEST_ERROR",
                            })
                        break
                        
                except KeyError as e:
                    error_msg = str(e)
                    
                    # Content filtering KeyError ì²˜ë¦¬
                    if "'choices'" in error_msg or "prompt_filter_results" in error_msg:
                        logger.warning(f"Content filtering KeyError - marking batch as FILTERED")
                        for qna in mini_batch:
                            results.append({
                                "category": qna["category"],
                                "answer": qna["answer"],
                                "pred": "FILTERED",
                                "response": "CONTENT_FILTER_ERROR",
                            })
                        break  # ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ë°°ì¹˜ë¡œ
                    else:
                        # ë‹¤ë¥¸ KeyErrorëŠ” ì¼ë°˜ ì²˜ë¦¬
                        logger.error(f"KeyError in batch processing: {e}")
                        retries += 1
                        if retries > max_retries:
                            for qna in mini_batch:
                                results.append({
                                    "category": qna["category"],
                                    "answer": qna["answer"],
                                    "pred": "FAILED",
                                    "response": f"KEYERROR: {str(e)}",
                                })
                            break
                        time.sleep(2 ** retries)
                        
                except Exception as e:
                    error_msg = str(e)
                    
                    # Content filtering ì¼ë°˜ Exception ì²˜ë¦¬
                    if "prompt_filter_results" in error_msg or "Response missing `choices` key" in error_msg:
                        logger.warning(f"Content filtering Exception - marking batch as FILTERED")
                        for qna in mini_batch:
                            results.append({
                                "category": qna["category"],
                                "answer": qna["answer"],
                                "pred": "FILTERED",
                                "response": "CONTENT_FILTER_ERROR",
                            })
                        break  # ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ë°°ì¹˜ë¡œ
                    else:
                        logger.error(f"Error processing batch: {e}")
                        retries += 1
                        if retries > max_retries:
                            for qna in mini_batch:
                                results.append({
                                    "category": qna["category"],
                                    "answer": qna["answer"],
                                    "pred": "FAILED",
                                    "response": f"ERROR: {str(e)}",
                                })
                            break
                        time.sleep(2 ** retries)
        
        return results
        
    except Exception as e:
        logger.error(f"Error in process_batch_streaming: {e}")
        return []


def process_category_streaming(category_info):
    """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
    category, hf_dataset_id, model_config, few_shots_config, is_hard, is_debug, num_debug_samples, template_type, csv_path = category_info
    
    logger.info(f"Processing category {category} in process {os.getpid()}")
    
    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        ds_dict = load_dataset(hf_dataset_id, category)
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±
        few_shots_prompt = None
        if few_shots_config['use_few_shots']:
            ds_dev = ds_dict["dev"]
            ds_dev = ds_dev.map(lambda x: {"answer": map_answer(x["answer"])})
            if is_hard:
                ds_dev = ds_dev.map(lambda x: {"category": convert_to_pascal_case(x["category"])})
            else:
                ds_dev = ds_dev.rename_column("Category", "category")
            few_shots_prompt = generate_few_shots_prompt(ds_dev)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        ds = ds_dict["test"]
        ds = ds.map(lambda x: {"answer": map_answer(x["answer"])})
        if is_hard:
            ds = ds.map(lambda x: {"category": convert_to_pascal_case(x["category"])})
        else:
            ds = ds.rename_column("Category", "category")
        
        if is_debug:
            ds = ds.select(range(num_debug_samples))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ í¬ê¸° ì„¤ì •
        chunk_size = model_config['batch_size'] * 10  # ë°°ì¹˜ í¬ê¸°ì˜ 10ë°°ì”© ì²˜ë¦¬
        total_items = len(ds)
        category_responses = []
        
        logger.info(f"Processing {total_items} items for category {category} in chunks of {chunk_size}")
        
        # ì²­í¬ë³„ ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€
        total_chunks = (total_items + chunk_size - 1) // chunk_size
        with tqdm(total=total_chunks, desc=f"Processing {category}", position=0, leave=True) as pbar:
            # ì²­í¬ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            for start_idx in range(0, total_items, chunk_size):
                end_idx = min(start_idx + chunk_size, total_items)
                chunk_ds = ds.select(range(start_idx, end_idx))
                
                # ì²­í¬ë¥¼ ë°°ì¹˜ ë°ì´í„°ë¡œ ë³€í™˜
                chunk_batch = [
                    {
                        "category": category,
                        "question": get_prompt(x, few_shots_prompt),
                        "answer": get_answer(x),
                    }
                    for x in chunk_ds
                ]
                
                # ì²­í¬ ì²˜ë¦¬
                chunk_results = process_batch_streaming(chunk_batch, model_config, few_shots_prompt, template_type)
                category_responses.extend(chunk_results)
                
                # ì²­í¬ ì²˜ë¦¬ í›„ ëœë¤ sleep (0-1ì´ˆ)
                sleep_time = random.uniform(0, 1)
                time.sleep(sleep_time)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                pbar.update(1)
                pbar.set_postfix({
                    'items': f"{len(category_responses)}/{total_items}",
                    'chunk_size': len(chunk_results)
                })
                
                # ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                if len(category_responses) >= 1000:
                    save_results_incremental(category_responses, csv_path)
                    logger.info(f"Intermediate save for category {category}: {len(category_responses)} items")
                    category_responses = []
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥
        if category_responses:
            save_results_incremental(category_responses, csv_path)
        
        logger.info(f"Completed category {category}")
        return category, "completed"
        
    except Exception as e:
        logger.error(f"Error processing category {category}: {e}")
        return category, f"error: {str(e)}"


def benchmark_multiprocess(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    is_debug = args.is_debug
    few_shots = "5shot" if args.use_few_shots else "0shot"
    
    if args.is_hard:
        hf_dataset_id = "HAERAE-HUB/KMMLU-HARD"
        dataset_name = "KMMLU-HARD"
        all_kmmlu_categories = [
            "accounting", "agricultural_sciences", "aviation_engineering_and_maintenance",
            "biology", "chemical_engineering", "chemistry", "civil_engineering",
            "computer_science", "construction", "criminal_law", "ecology", "economics",
            "education", "electrical_engineering", "electronics_engineering",
            "energy_management", "environmental_science", "fashion", "food_processing",
            "gas_technology_and_engineering", "geomatics", "health", "industrial_engineer",
            "information_technology", "interior_architecture_and_design", "korean_history",
            "law", "machine_design_and_manufacturing", "management", "maritime_engineering",
            "marketing", "materials_engineering", "math", "mechanical_engineering",
            "nondestructive_testing", "patent", "political_science_and_sociology",
            "psychology", "public_safety", "railway_and_automotive_engineering",
            "real_estate", "refrigerating_machinery", "social_welfare", "taxation",
            "telecommunications_and_wireless_technology",
        ]
    else:
        hf_dataset_id = "HAERAE-HUB/KMMLU"
        dataset_name = "KMMLU"
        all_kmmlu_categories = [
            "Accounting", "Agricultural-Sciences", "Aviation-Engineering-and-Maintenance",
            "Biology", "Chemical-Engineering", "Chemistry", "Civil-Engineering",
            "Computer-Science", "Construction", "Criminal-Law", "Ecology", "Economics",
            "Education", "Electrical-Engineering", "Electronics-Engineering",
            "Energy-Management", "Environmental-Science", "Fashion", "Food-Processing",
            "Gas-Technology-and-Engineering", "Geomatics", "Health", "Industrial-Engineer",
            "Information-Technology", "Interior-Architecture-and-Design", "Korean-History",
            "Law", "Machine-Design-and-Manufacturing", "Management", "Maritime-Engineering",
            "Marketing", "Materials-Engineering", "Math", "Mechanical-Engineering",
            "Nondestructive-Testing", "Patent", "Political-Science-and-Sociology",
            "Psychology", "Public-Safety", "Railway-and-Automotive-Engineering",
            "Real-Estate", "Refrigerating-Machinery", "Social-Welfare", "Taxation",
            "Telecommunications-and-Wireless-Technology",
        ]
    
    # ëª¨ë¸ ì„¤ì •
    model_name = os.getenv("MODEL_NAME", "gpt-5-mini")
    model_version = os.getenv("MODEL_VERSION", "2025-08-08")
    
    model_config = {
        'provider': args.model_provider,
        'hf_model_id': args.hf_model_id,
        'batch_size': args.batch_size,
        'max_tokens': args.max_tokens,
        'temperature': args.temperature,
        'max_retries': args.max_retries,
        'wait_time': args.wait_time,
    }
    
    few_shots_config = {
        'use_few_shots': args.use_few_shots
    }
    
    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    os.makedirs("results", exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„±
    if args.use_few_shots:
        if args.is_hard:
            csv_path = f"results/[KMMLU-HARD] {model_name}-{model_version}-5shot.csv"
        else:
            csv_path = f"results/[KMMLU] {model_name}-{model_version}-5shot.csv"
    else:
        if args.is_hard:
            csv_path = f"results/[KMMLU-HARD] {model_name}-{model_version}-0shot.csv"
        else:
            csv_path = f"results/[KMMLU] {model_name}-{model_version}-0shot.csv"
    
    abs_csv_path = os.path.abspath(csv_path)
    
    logger.info(f"ğŸ¯ Target output file: {abs_csv_path}")
    
    # ê¸°ì¡´ íŒŒì¼ ìƒíƒœ í™•ì¸
    if os.path.exists(csv_path):
        file_size = os.path.getsize(csv_path)
        existing_df = pd.read_csv(csv_path)
        logger.info(f"ğŸ“‹ Found existing file with {len(existing_df)} records ({file_size:,} bytes)")
    else:
        logger.info(f"ğŸ“‹ No existing file found - will create new file")
    
    # ì‹¤í–‰í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
    if args.categories and args.categories != ['']:
        # ì‚¬ìš©ìê°€ ì§€ì •í•œ ì¹´í…Œê³ ë¦¬ë“¤ ê²€ì¦
        invalid_categories = [c for c in args.categories if c not in all_kmmlu_categories]
        if invalid_categories:
            logger.error(f"Invalid categories specified: {invalid_categories}")
            logger.error(f"Available categories: {all_kmmlu_categories}")
            return
        selected_categories = args.categories
        logger.info(f"ğŸ¯ Processing user-specified categories: {selected_categories}")
    else:
        selected_categories = all_kmmlu_categories
        logger.info(f"ğŸ¯ Processing all categories: {selected_categories}")
    
    # ì™„ë£Œëœ ì¹´í…Œê³ ë¦¬ í™•ì¸
    completed_categories = get_completed_categories(csv_path)
    
    # ë‚¨ì€ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (ì™„ë£Œë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬)
    remaining_categories = [c for c in selected_categories if c not in completed_categories]
    
    if not remaining_categories:
        logger.info("All specified categories already completed!")
        return
    
    logger.info(f"Processing {len(remaining_categories)} remaining categories: {remaining_categories}")
    logger.info(f"Using multiprocessing with {args.max_workers} workers")
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‘ì—… ì¤€ë¹„
    category_tasks = [
        (category, hf_dataset_id, model_config, few_shots_config, args.is_hard, 
         is_debug, args.num_debug_samples, args.template_type, csv_path)
        for category in remaining_categories
    ]
    
    start_time = time.time()
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰ - tqdm ê°œì„ 
    with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ submit ì‚¬ìš©
        future_to_category = {
            executor.submit(process_category_streaming, task): task[0] 
            for task in category_tasks
        }
        
        completed_count = 0
        # ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 
        with tqdm(total=len(remaining_categories), desc="Categories", position=1, leave=True) as category_pbar:
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    # 200ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„)
                    result_category, status = future.result(timeout=200)
                    completed_count += 1
                    logger.info(f"Category {result_category} completed: {status} ({completed_count}/{len(remaining_categories)})")
                    



                    # ì¹´í…Œê³ ë¦¬ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    category_pbar.update(1)
                    category_pbar.set_postfix({
                        'current': result_category,
                        'status': status
                    })
                
                except TimeoutError:
                    logger.error(f"Category {category} timed out after 30 minutes")
                    completed_count += 1
                    category_pbar.update(1)
                    category_pbar.set_postfix({



                        
                        'current': category,
                        'status': 'timeout'
                    })
                    
                except Exception as e:
                    logger.error(f"Category {category} failed with exception: {e}")
                    completed_count += 1
                    category_pbar.update(1)
                    category_pbar.set_postfix({
                        'current': category,
                        'status': 'failed'
                    })

    end_time = time.time()
    total_time = format_timespan(end_time - start_time)
    
    logger.info(f"====== [DONE] All specified categories processed in {total_time} =====")
    
    # ìµœì¢… íŒŒì¼ ìƒíƒœ í™•ì¸
    if os.path.exists(csv_path):
        final_df = pd.read_csv(csv_path)
        final_file_size = os.path.getsize(csv_path)
        logger.info(f"ğŸ Final output file status:")
        logger.info(f"   - Path: {abs_csv_path}")
        logger.info(f"   - Records: {len(final_df)}")
        logger.info(f"   - Size: {final_file_size:,} bytes ({final_file_size/(1024*1024):.2f} MB)")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_counts = final_df['category'].value_counts()
        logger.info(f"   - Completed categories: {len(category_counts)}")
        logger.info(f"   - Records per category: {dict(category_counts)}")
    else:
        logger.error(f"âŒ Final output file not found: {abs_csv_path}")
    
    # ìµœì¢… í‰ê°€
    logger.info(f"====== [START] Final Evaluation - CSV_PATH: {csv_path} =====")
    evaluate(csv_path)
    logger.info(f"====== [END] Evaluation completed =====")


def benchmark_sequential(args):
    """ê¸°ì¡´ ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹"""
    logger.info("Using sequential processing")
    # ê¸°ì¡´ benchmark í•¨ìˆ˜ ë¡œì§ì„ ì—¬ê¸°ì— ë³µì‚¬
    # ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)


def evaluate_each_category(responses, category):
    df = pd.DataFrame(responses)
    df = df[df["category"] == category]
    # FAILED ì‘ë‹µ í•„í„°ë§
    df = df[df["pred"] != "FAILED"]
    if df.empty:
        return 0.0
    df["correct"] = df["answer"] == df["pred"]
    acc = round(df["correct"].mean() * 100, 2)
    return acc


def evaluate(csv_path):
    abs_csv_path = os.path.abspath(csv_path)
    
    if not os.path.exists(csv_path):
        logger.error(f"âŒ CSV file does not exist: {abs_csv_path}")
        return
    
    logger.info(f"ğŸ“Š Starting evaluation of: {abs_csv_path}")
    
    result = pd.read_csv(csv_path)
    if result.empty:
        logger.error(f"âŒ CSV file is empty: {abs_csv_path}")
        return
    
    logger.info(f"ğŸ“Š Loaded {len(result)} records for evaluation")
    
    # FAILED ì‘ë‹µ í•„í„°ë§ ë° ë¡œê¹…
    original_count = len(result)
    failed_count = len(result[result["pred"] == "FAILED"])
    if failed_count > 0:
        logger.warning(f"Found {failed_count} FAILED responses out of {original_count} total responses")
        logger.info(f"Excluding FAILED responses from accuracy calculation")
        result = result[result["pred"] != "FAILED"]
        logger.info(f"Evaluating on {len(result)} valid responses")
    
    result["correct"] = result["answer"] == result["pred"]

    category_avg = (
        result.groupby(["category"])
        .agg(correct_mean=("correct", "mean"), correct_count=("correct", "size"))
        .reset_index()
    )
    print(category_avg)
    overall_avg = category_avg["correct_mean"].mean()
    print(f"Overall Average: {overall_avg}")

    os.makedirs("evals", exist_ok=True)
    filename = csv_path.split("/")[-1].split(".")[0]
    
    eval_file = f"evals/{filename}-eval.csv"
    category_avg.to_csv(eval_file, index=False)
    
    abs_eval_file = os.path.abspath(eval_file)
    
    logger.info(f"âœ… Evaluation results saved:")
    logger.info(f"   - Results file: {abs_eval_file}")
    logger.info(f"   - Categories evaluated: {len(category_avg)}")
    logger.info(f"   - Overall accuracy: {overall_avg:.2%}")


if __name__ == "__main__":
    dotenv_path = os.getenv('DOTENV_PATH', '.env')
    load_dotenv(dotenv_path, override=True)
   
    parser = argparse.ArgumentParser(description="KMMLU Benchmark with Multiprocessing and Streaming")

    parser.add_argument("--is_debug", type=str2bool, default=False)
    parser.add_argument("--num_debug_samples", type=int, default=10)
    parser.add_argument("--model_provider", type=str, default="azureopenai")
    parser.add_argument("--hf_model_id", type=str, default="microsoft/Phi-3.5-mini-instruct")
    parser.add_argument("--batch_size", type=int, default=20)
    parser.add_argument("--max_retries", type=int, default=2)
    parser.add_argument("--max_tokens", type=int, default=1024)
    parser.add_argument("--temperature", type=float, default=0.01)
    parser.add_argument("--template_type", type=str, default="basic")
    parser.add_argument("--is_hard", type=str2bool, default=True)
    parser.add_argument("--use_few_shots", type=str2bool, default=True)
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë“¤ë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì¸ìˆ˜
    parser.add_argument(
        "--categories", 
        type=str, 
        nargs='*', 
        default=None, 
        help='Specific categories to process (e.g., --categories "accounting" "biology")'
    )
    
    # ìƒˆë¡œìš´ ë©€í‹°í”„ë¡œì„¸ì‹± ê´€ë ¨ ì¸ìˆ˜
    parser.add_argument("--use_multiprocessing", type=str2bool, default=True, help="Enable multiprocessing")
    parser.add_argument("--max_workers", type=int, default=3, help="Maximum number of worker processes")
    parser.add_argument("--wait_time", type=float, default=1.0, help="Wait time between Bedrock requests to avoid throttling")

    args = parser.parse_args()

    valid_providers = ["azureopenai", "openai", "azureml", "azureai", "huggingface", "bedrock"]
    assert args.model_provider in valid_providers, f"Invalid 'model_provider' value. Please choose from {valid_providers}."

    valid_template_types = ["basic", "chat", "gpt5"]
    assert args.template_type in valid_template_types, f"Invalid 'template_type' value. Please choose from {valid_template_types}."

    # ì¹´í…Œê³ ë¦¬ ì¸ìˆ˜ ë¡œê¹…
    if args.categories and args.categories != ['']:
        logger.info(f"ğŸ¯ User specified categories: {args.categories}")
    else:
        logger.info(f"ğŸ¯ Will process all available categories")

    logger.info(args)
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‹¤í–‰ ë°©ì‹ ì„ íƒ
    if args.use_multiprocessing and args.max_workers > 1:
        benchmark_multiprocess(args)
    else:
        benchmark_sequential(args)